# -*- coding: utf-8 -*-
"""Unet_DMSegmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16uOKqXFRKX_RuUhFIQPdoqNSGTpjaWSu
"""

pip install denoising_diffusion_pytorch

from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer
import kagglehub
import matplotlib.pyplot as plt
import random
import os
from PIL import Image, ImageEnhance
import numpy as np
import torch

# Download latest version
path = kagglehub.dataset_download("mateuszbuda/lgg-mri-segmentation")

print("Path to dataset files:", path)

SEED = 42
DATA_DIR = Path("/kaggle/input/lgg-mri-segmentation/kaggle_3m")
IMG_SIZE = (256, 256)
BATCH_SIZE = 8

# @title
# import os
# import glob

# # Define the original dataset path and a new temporary directory
# original_dataset_path = '/kaggle/input/lgg-mri-segmentation/kaggle_3m'
# processed_image_folder = './processed_lgg_mri_images'

# # Create the processed image folder if it doesn't exist
# os.makedirs(processed_image_folder, exist_ok=True)

# # Find all .tif files recursively and create symbolic links
# image_files = glob.glob(os.path.join(original_dataset_path, '**', '*.tif'), recursive=True)

# print(f"Found {len(image_files)} .tif images in the original dataset.")

# # Create symbolic links for each image into the processed_image_folder
# for i, img_path in enumerate(image_files):
#     # Get the base filename and construct a unique link name
#     # Using a simple counter to avoid name collisions if filenames are not unique across subfolders
#     link_name = os.path.join(processed_image_folder, f"image_{i}.tif")
#     try:
#         # Using os.link for hard links or os.symlink for symbolic links
#         # Symlinks are generally preferred to avoid duplicating data on disk
#         os.symlink(img_path, link_name)
#     except FileExistsError:
#         # Handle cases where the link might already exist (e.g., re-running cell)
#         pass
#     except OSError as e:
#         print(f"Could not create symbolic link for {img_path} to {link_name}: {e}")

# print(f"Successfully created symbolic links for images in '{processed_image_folder}'")

# # Verify the number of images in the processed folder
# num_processed_images = len(os.listdir(processed_image_folder))
# print(f"Number of images in the processed folder: {num_processed_images}")

# # Now, update the Trainer to use this new folder
# # This path will be used in the next cell where Trainer is initialized.
# # Let's save it to a variable that can be accessed later.
# trainer_data_folder = processed_image_folder

"""how to convert `.tif` files to `.png` files using the `Pillow` library."""

# @title
# from PIL import Image

# # Define the input folder containing .tif images and the output folder for .png images
# input_folder = './processed_lgg_mri_images' # This is where your .tif symlinks are
# output_folder = './converted_lgg_mri_images_png'

# # Create the output folder if it doesn't exist
# os.makedirs(output_folder, exist_ok=True)

# # Find all .tif files in the input folder
# tif_files = glob.glob(os.path.join(input_folder, '*.tif'))

# print(f"Found {len(tif_files)} .tif images to convert.")

# # Convert each .tif image to .png
# for i, tif_path in enumerate(tif_files):
#     try:
#         # Open the .tif image
#         img = Image.open(tif_path)

#         # Convert to RGB if not already (important for consistency with PNG/JPG)
#         if img.mode != 'RGB':
#             img = img.convert('RGB')

#         # Construct the output filename with .png extension
#         base_filename = os.path.basename(tif_path) # e.g., 'image_0.tif'
#         output_filename = os.path.splitext(base_filename)[0] + '.png' # e.g., 'image_0.png'
#         output_path = os.path.join(output_folder, output_filename)

#         # Save the image as .png
#         img.save(output_path, 'PNG')

#         if (i + 1) % 100 == 0:
#             print(f"Converted {i + 1}/{len(tif_files)} images...")

#     except Exception as e:
#         print(f"Error converting {tif_path}: {e}")

# print(f"Conversion complete! Converted images saved to '{output_folder}'")
# print(f"Number of .png images in the output folder: {len(os.listdir(output_folder))}")

# # Now, update trainer_data_folder to use this new folder
# trainer_data_folder = output_folder
# print(f"Updated trainer_data_folder to: {trainer_data_folder}")

import os
import glob

# Define the original dataset path (this variable already exists in the kernel)
original_dataset_path = '/kaggle/input/lgg-mri-segmentation/kaggle_3m'

# Initialize lists to store image and mask paths
image_paths = []
mask_paths = []

# Iterate all .tif files in subfolders recursively
for img_path_obj in glob.glob(os.path.join(original_dataset_path, '**', '*.tif'), recursive=True):
    p = str(img_path_obj)

    # Skip mask files when identifying the main image
    if p.endswith('_mask.tif'):
        continue

    # Create mask path by adding _mask before .tif
    mask_path = p.replace(".tif", "_mask.tif")

    # Check if the corresponding mask file exists
    if os.path.exists(mask_path):
        image_paths.append(p)
        mask_paths.append(mask_path)

print(f"Total pairs found: {len(image_paths)}")

# Make sure lists are aligned by zipping them
pairs = list(zip(image_paths, mask_paths))

# Sort the pairs for deterministic ordering (optional but good practice)
pairs = sorted(pairs)

print("Example of first 3 pairs:")
for i, (img_p, mask_p) in enumerate(pairs[:3]):
    print(f"  Image: {img_p}\n  Mask: {mask_p}")

from collections import defaultdict
from pathlib import Path
import random
from sklearn.model_selection import train_test_split

grouped = defaultdict(list)
for img, msk in pairs:
    parent = Path(img).parent.name
    grouped[parent].append((img, msk))

# create list of groups then split
group_items = list(grouped.items())
random.seed(SEED)
random.shuffle(group_items)

# split groups ~ 85% train / 15% val
train_groups, val_groups = train_test_split(group_items, test_size=0.15, random_state=SEED)
train_pairs = [pair for _, pairs_list in train_groups for pair in pairs_list]
val_pairs = [pair for _, pairs_list in val_groups for pair in pairs_list]

print("Train pairs:", len(train_pairs))
print("Val pairs:", len(val_pairs))

def load_image(path, size=IMG_SIZE):
    img = Image.open(path).convert("L")  # grayscale
    img = img.resize(size, Image.BILINEAR)
    arr = np.array(img).astype(np.float32)
    # normalize by max (avoid dividing by zero)
    mx = arr.max() if arr.max() > 0 else 1.0
    arr = arr / mx
    # expand channel
    arr = np.expand_dims(arr, axis=-1)
    return arr

def load_mask(path, size=IMG_SIZE):
    m = Image.open(path).convert("L")
    m = m.resize(size, Image.NEAREST)
    arr = np.array(m).astype(np.float32)
    # Binarize: any non-zero -> 1
    arr = (arr > 0).astype(np.float32)
    arr = np.expand_dims(arr, axis=-1)
    return arr

def random_flip_rotate(image, mask):
    # image, mask are numpy arrays
    if random.random() < 0.5:
        image = np.flipud(image)
        mask = np.flipud(mask)
    if random.random() < 0.5:
        image = np.fliplr(image)
        mask = np.fliplr(mask)
    # random 90-degree rotations
    k = random.randint(0, 3)
    if k:
        image = np.rot90(image, k)
        mask = np.rot90(mask, k)
    return image.copy(), mask.copy()

from tensorflow.keras.utils import Sequence
from tensorflow.keras import layers
import tensorflow.keras as keras

class MRIDataset(Sequence):
    def __init__(self, pairs, batch_size=BATCH_SIZE, img_size=IMG_SIZE, augment=False, shuffle=True):
        self.pairs = pairs
        self.batch_size = batch_size
        self.img_size = img_size
        self.augment = augment
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        return int(np.ceil(len(self.pairs) / float(self.batch_size)))

    def on_epoch_end(self):
        self.indexes = np.arange(len(self.pairs))
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def __getitem__(self, idx):
        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_pairs = [self.pairs[i] for i in batch_indexes]
        images = []
        masks = []
        for img_path, mask_path in batch_pairs:
            img = load_image(img_path, size=self.img_size)
            mask = load_mask(mask_path, size=self.img_size)
            if self.augment:
                img, mask = random_flip_rotate(img, mask)
            images.append(img)
            masks.append(mask)
        images = np.stack(images, axis=0)
        masks = np.stack(masks, axis=0)
        return images, masks

train_gen = MRIDataset(train_pairs, batch_size=BATCH_SIZE, augment=True, shuffle=True)
val_gen = MRIDataset(val_pairs, batch_size=BATCH_SIZE, augment=False, shuffle=False)

# show some examples
x, y = train_gen[0]
print("Batch shapes:", x.shape, y.shape)

def show_sample(img, mask, title=None):
    fig, ax = plt.subplots(1,2, figsize=(8,4))
    ax[0].imshow(img.squeeze(), cmap='gray')
    ax[0].set_title("Image")
    ax[1].imshow(mask.squeeze(), cmap='gray')
    ax[1].set_title("Mask")
    for a in ax:
        a.axis('off')
    if title:
        plt.suptitle(title)
    plt.show()

for i in range(min(3, x.shape[0])):
    show_sample(x[i], y[i], title=f"Train sample {i}")

def conv_block(x, filters, kernel_size=3, padding="same", activation="relu"):
    x = layers.Conv2D(filters, kernel_size, padding=padding)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation(activation)(x)
    x = layers.Conv2D(filters, kernel_size, padding=padding)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation(activation)(x)
    return x

def encoder_block(x, filters):
    c = conv_block(x, filters)
    p = layers.MaxPooling2D((2,2))(c)
    return c, p

def decoder_block(x, skip, filters):
    us = layers.Conv2DTranspose(filters, (2,2), strides=(2,2), padding="same")(x)
    concat = layers.concatenate([us, skip])
    c = conv_block(concat, filters)
    return c

def build_unet(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 1)):
    inputs = layers.Input(shape=input_shape)
    # Encoder
    c1, p1 = encoder_block(inputs, 32)
    c2, p2 = encoder_block(p1, 64)
    c3, p3 = encoder_block(p2, 128)
    c4, p4 = encoder_block(p3, 256)
    # Bottleneck
    bn = conv_block(p4, 512)
    # Decoder
    d1 = decoder_block(bn, c4, 256)
    d2 = decoder_block(d1, c3, 128)
    d3 = decoder_block(d2, c2, 64)
    d4 = decoder_block(d3, c1, 32)
    outputs = layers.Conv2D(1, (1,1), activation="sigmoid")(d4)
    model = keras.Model(inputs, outputs, name="UNet")
    return model

model = build_unet(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 1))
model.summary()

from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from tensorflow.keras import backend as K
import tensorflow as tf

# Custom Loss and Metrics for Segmentation
def dice_coef(y_true, y_pred, smooth=1e-6):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

def iou_metric(y_true, y_pred, smooth=1e-6):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    union = K.sum(y_true_f) + K.sum(y_pred_f) - intersection
    return (intersection + smooth) / (union + smooth)

CHECKPOINT_PATH = "/kaggle/working/unet_lgg_checkpoint.h5"
checkpoint = ModelCheckpoint(CHECKPOINT_PATH, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1, min_lr=1e-7)
early = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)

EPOCHS = 30

# Compile the model
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy', dice_coef, iou_metric]
)

history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=EPOCHS,
    callbacks=[checkpoint, reduce_lr, early] # Added callbacks
)

def plot_history(history):
    hist = history.history
    epochs = range(1, len(hist['loss'])+1)
    plt.figure(figsize=(14,10))

    plt.subplot(2,2,1)
    plt.plot(epochs, hist['loss'], label='train_loss')
    plt.plot(epochs, hist['val_loss'], label='val_loss')
    plt.legend(); plt.title('Loss')

    plt.subplot(2,2,2)
    plt.plot(epochs, hist['dice_coef'], label='train_dice')
    plt.plot(epochs, hist['val_dice_coef'], label='val_dice')
    plt.legend(); plt.title('Dice Coef')

    plt.subplot(2,2,3)
    plt.plot(epochs, hist['iou_metric'], label='train_iou')
    plt.plot(epochs, hist['val_iou_metric'], label='val_iou')
    plt.legend(); plt.title('IoU')

    plt.subplot(2,2,4)
    plt.plot(epochs, hist['accuracy'], label='train_acc')
    plt.plot(epochs, hist['val_accuracy'], label='val_acc')
    plt.legend(); plt.title('Accuracy')
    plt.show()

plot_history(history)

"""Between epoch 23-26 appears to be good cutoff for training."""

# @title
# load best weights
if os.path.exists(CHECKPOINT_PATH):
    print("Loading best weights from", CHECKPOINT_PATH)
    model.load_weights(CHECKPOINT_PATH)

# aggregate metrics manually over validation set
dice_vals = []
iou_vals = []
acc_vals = []
count = 0

for i in range(len(val_gen)):
    imgs, msks = val_gen[i]
    preds = model.predict(imgs)
    preds_bin = (preds >= 0.5).astype(np.float32)
    # compute per-sample metrics
    for j in range(imgs.shape[0]):
        y_true = msks[j]
        y_pred = preds_bin[j]
        # flatten
        y_true_f = y_true.flatten()
        y_pred_f = y_pred.flatten()
        # dice
        inter = np.sum(y_true_f * y_pred_f)
        dice = (2. * inter + 1e-6) / (np.sum(y_true_f) + np.sum(y_pred_f) + 1e-6)
        # iou
        union = np.sum(y_true_f) + np.sum(y_pred_f) - inter
        iou = (inter + 1e-6) / (union + 1e-6)
        acc = (y_true_f == y_pred_f).mean()
        dice_vals.append(dice)
        iou_vals.append(iou)
        acc_vals.append(acc)
        count += 1

print("Val samples:", count)
print("Mean Dice:", np.mean(dice_vals))
print("Mean IoU:", np.mean(iou_vals))
print("Mean Pixel Accuracy:", np.mean(acc_vals))



"""To prepare the diffusion training data, I will convert the original '.tif' MRI images from the 'lgg-mri-segmentation' dataset to '.png' format, resize them to `128x128` (the input size expected by the diffusion model), and save them into a new directory named 'converted_lgg_mri_images_png'. This directory will then be used as the `trainer_data_folder` for the diffusion model training."""

pip install denoising_diffusion_pytorch

from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer
import kagglehub
import matplotlib.pyplot as plt
import random
import os
from PIL import Image, ImageEnhance
import numpy as np
import torch
from pathlib import Path

# Download latest version
path = kagglehub.dataset_download("mateuszbuda/lgg-mri-segmentation")

print("Path to dataset files:", path)

SEED = 42
DATA_DIR = Path("/kaggle/input/lgg-mri-segmentation/kaggle_3m")
IMG_SIZE = (256, 256)
BATCH_SIZE = 8

import os

# The 'path' variable holds the base directory for the dataset
base_dataset_path = path

print(f"Listing contents of base dataset path: {base_dataset_path}")

# List all items (files and directories) in the base_dataset_path
if os.path.exists(base_dataset_path):
    for item in os.listdir(base_dataset_path):
        item_path = os.path.join(base_dataset_path, item)
        if os.path.isdir(item_path):
            print(f"  [DIR] {item}")
        else:
            print(f"  [FILE] {item}")
else:
    print(f"Base dataset path does not exist: {base_dataset_path}")

import os
import glob
from pathlib import Path

# Correctly define the original_dataset_path using the 'path' variable
original_dataset_path = Path(path) / "kaggle_3m"

# Initialize lists to store image and mask paths
image_paths = []
mask_paths = []

# Iterate all .tif files in subfolders recursively
for img_path_obj in glob.glob(os.path.join(original_dataset_path, '**', '*.tif'), recursive=True):
    p = str(img_path_obj)

    # Skip mask files when identifying the main image
    if p.endswith('_mask.tif'):
        continue

    # Create mask path by adding _mask before .tif
    mask_path = p.replace(".tif", "_mask.tif")

    # Check if the corresponding mask file exists
    if os.path.exists(mask_path):
        image_paths.append(p)
        mask_paths.append(mask_path)

print(f"Total pairs found: {len(image_paths)}")

# Make sure lists are aligned by zipping them
pairs = list(zip(image_paths, mask_paths))

# Sort the pairs for deterministic ordering (optional but good practice)
pairs = sorted(pairs)

print("Example of first 3 pairs:")
for i, (img_p, mask_p) in enumerate(pairs[:3]):
    print(f"  Image: {img_p}\n  Mask: {mask_p}")

from PIL import Image
import os

# Define the target size for diffusion model
IMG_SIZE_DIFFUSION = (128, 128)

# Define the output folder for converted PNG images
output_folder = './converted_lgg_mri_images_png'

# Create the output folder if it doesn't exist
os.makedirs(output_folder, exist_ok=True)

print(f"Found {len(image_paths)} original .tif images to convert (from previous step).")

# Iterate through the list of original image paths
for i, img_path in enumerate(image_paths):
    try:
        # Open the .tif image
        img = Image.open(img_path)

        # Convert to RGB mode
        if img.mode != 'RGB':
            img = img.convert('RGB')

        # Resize the image to 128x128 using Image.BILINEAR
        img_resized = img.resize(IMG_SIZE_DIFFUSION, Image.BILINEAR)

        # Construct the output filename with .png extension
        base_filename = os.path.basename(img_path) # e.g., 'TCGA_CS_4941_19960909_1.tif'
        output_filename = os.path.splitext(base_filename)[0] + '.png' # e.g., 'TCGA_CS_4941_19960909_1.png'
        output_path = os.path.join(output_folder, output_filename)

        # Save the image as .png
        img_resized.save(output_path, 'PNG')

        if (i + 1) % 100 == 0:
            print(f"Converted {i + 1}/{len(image_paths)} images...")

    except Exception as e:
        print(f"Error converting {img_path}: {e}")

print(f"Conversion complete! Converted images saved to '{output_folder}'")
print(f"Number of .png images in the output folder: {len(os.listdir(output_folder))}")

# Update trainer_data_folder to point to this new folder for the diffusion model training
trainer_data_folder = output_folder
print(f"Updated trainer_data_folder to: {trainer_data_folder}")

"""## Train Diffusion Model
Train the Denoising Diffusion Probabilistic Model (DDPM) on the prepared dataset of MRI images, ensuring the `trainer_data_folder` variable points to the newly created directory of 128x128 PNG images.

"""

import torch
import os
from PIL import Image
import numpy as np
from pytorch_fid import fid_score

# 1. Define checkpoint steps and initialize FID scores dictionary
checkpoint_steps = [1000, 10000, 20000, 30000, 40000, 50000]
fid_scores = {}

# Directory for saving model checkpoints to Google Drive
GD_SAVE_DIR = '/content/drive/MyDrive/ColabNotebooks/EnhanceMRIdata'
os.makedirs(GD_SAVE_DIR, exist_ok=True)

print(f"Checkpoint steps defined: {checkpoint_steps}")
print(f"FID scores dictionary initialized: {fid_scores}")
print(f"Google Drive save directory: {GD_SAVE_DIR}")

from google.colab import drive
drive.mount('/content/drive')

# @title
import torch
import os
from PIL import Image
import numpy as np
from pytorch_fid import fid_score
import shutil
from tqdm.auto import tqdm # Added import
import matplotlib.pyplot as plt # Added import
import math

# Define the show_images function
def show_images(images, cols=4):
    rows = math.ceil(len(images) / cols)
    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))
    axes = axes.flatten() # Flatten the 2D array of axes for easy iteration

    for i, img in enumerate(images):
        # Convert PyTorch tensor to NumPy array, permute to HxWxC, and normalize to [0, 1]
        img_np = img.permute(1, 2, 0).cpu().numpy()
        img_np = (img_np + 1) / 2 # Scale from [-1, 1] to [0, 1]

        # Ensure image is grayscale if it has one channel, or take mean for RGB to display well
        if img_np.shape[-1] == 1: # Grayscale
            axes[i].imshow(img_np.squeeze(), cmap='gray')
        else: # RGB
            axes[i].imshow(img_np)
        axes[i].axis('off')

    # Hide any unused subplots if the number of images is not a perfect multiple of cols
    for j in range(i + 1, len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    plt.show()

# 1. Define GD_SAVE_DIR as '/content/drive/MyDrive/ColabNotebooks/EnhanceMRIdata'
#    and ensure this directory exists. Create a subdirectory within it named 'real_images_for_fid'.
GD_SAVE_DIR = '/content/drive/MyDrive/ColabNotebooks/EnhanceMRIdata'
real_images_for_fid_dir = os.path.join(GD_SAVE_DIR, 'real_images_for_fid')

os.makedirs(GD_SAVE_DIR, exist_ok=True)
os.makedirs(real_images_for_fid_dir, exist_ok=True)

print(f"Google Drive save directory: {GD_SAVE_DIR}")
print(f"Real images for FID directory: {real_images_for_fid_dir}")

# 2. Prepare real images for FID calculation: Iterate through the original preprocessed
#    MRI images located in `trainer_data_folder`.
# 3. For each real image, open it using PIL, convert it to RGB mode if necessary, resize it to
#    `IMG_SIZE_DIFFUSION` (128x128) using `Image.BILINEAR`, and save it as a PNG file
#    into the 'real_images_for_fid' directory.

# Assuming trainer_data_folder is './converted_lgg_mri_images_png' and IMG_SIZE_DIFFUSION = (128, 128)

print(f"Preparing real images for FID from {trainer_data_folder}...")
real_image_files = [os.path.join(trainer_data_folder, f) for f in os.listdir(trainer_data_folder) if os.path.isfile(os.path.join(trainer_data_folder, f))]

for i, img_path in enumerate(real_image_files):
    try:
        img = Image.open(img_path)
        if img.mode != 'RGB':
            img = img.convert('RGB')
        img_resized = img.resize(IMG_SIZE_DIFFUSION, Image.BILINEAR)
        img_resized.save(os.path.join(real_images_for_fid_dir, f'real_fid_img_{i}.png'))
    except Exception as e:
        print(f"Error processing real image {img_path} for FID: {e}")
        continue

print(f"Finished preparing {len(real_image_files)} real images for FID.")

# Instantiate the Unet model, GaussianDiffusion model, and Trainer again for the fixed version
model = Unet(
    dim = 64,
    dim_mults = (1, 2, 4, 8),
    flash_attn = False, # Changed to False to address potential RuntimeError
    channels = 3 # Explicitly set channels to 3 for RGB images
)

diffusion = GaussianDiffusion(
    model,
    image_size = 128,
    timesteps = 1000,
    sampling_timesteps = 100
    # Removed channels = 3 as it's not a direct argument for GaussianDiffusion
)

trainer = Trainer(
    diffusion,
    trainer_data_folder,
    train_batch_size = 32,
    train_lr = 8e-5,
    train_num_steps = 50000,
    gradient_accumulate_every = 2,
    ema_decay = 0.995,
    amp = True,
    calculate_fid = False
    # Removed channels = 3 as it's not a direct argument for Trainer
)


# 5. Initialize an empty dictionary, `fid_scores`, to store FID values for each checkpoint step.
fid_scores = {}
print("FID scores dictionary initialized for custom loop.")

# Define checkpoint steps (from previous cell 77291149)
checkpoint_steps = [1000, 10000, 20000, 30000, 40000, 50000]

# Define logging frequency - using a default value as trainer.log_every_steps might not be directly accessible
log_steps_frequency = 1000 # Default value for log_every_steps in denoising_diffusion_pytorch Trainer

# 6. Implement a custom training loop:
with tqdm(range(trainer.train_num_steps), desc="Training Diffusion Model") as pbar:
    for step in pbar:
        trainer.model.train()

        # --- TRAINING STEP ---
        with trainer.accelerator.accumulate(trainer.model):
            data = next(trainer.dl)

            # FIX 1: Use trainer.model instead of diffusion to ensure Accelerate tracks gradients
            loss = trainer.model(data)

            trainer.accelerator.backward(loss)

            if trainer.max_grad_norm > 0:
                trainer.accelerator.clip_grad_norm_(trainer.model.parameters(), trainer.max_grad_norm)

            trainer.opt.step()
            trainer.opt.zero_grad()

        trainer.accelerator.wait_for_everyone()
        trainer.ema.update()

        # --- LOGGING ---
        if trainer.accelerator.is_main_process:
            pbar.set_postfix({'loss': loss.item()})
            if (step % log_steps_frequency) == 0:
                trainer.accelerator.print(f'{step}: {loss.item():.4f}')

            # --- CHECKPOINT AND FID ---
            # 7. Check if (step + 1) matches a checkpoint step
            if (step + 1) in checkpoint_steps:
                milestone = step + 1
                print(f"\n--- Checkpoint and FID calculation at step {milestone} ---")

                # a. Save the EMA model
                model_save_path = os.path.join(GD_SAVE_DIR, f'diffusion_model_step_{milestone}.pt')
                torch.save(trainer.ema.ema_model.state_dict(), model_save_path)
                print(f"EMA model saved to {model_save_path}")

                # b. Get the EMA model explicitly
                ema_diffusion_model = trainer.ema.ema_model
                ema_diffusion_model.eval()

                print("Generating sample images for FID...")

                # FIX 2: Use torch.no_grad() for safety during generation
                with torch.no_grad():
                    # Call sample on the EMA model directly
                    generated_images = ema_diffusion_model.sample(batch_size=16)

                show_images(generated_images, cols=4)

                # d. Convert to numpy for saving
                generated_images_np = ((generated_images + 1) * 127.5).clamp(0, 255).permute(0, 2, 3, 1).cpu().numpy().astype('uint8')

                # e. Create temp directory
                temp_gen_fid_dir = os.path.join(GD_SAVE_DIR, f'temp_gen_fid_step_{milestone}')
                os.makedirs(temp_gen_fid_dir, exist_ok=True)

                # f. Save images
                for img_idx, img_arr in enumerate(generated_images_np):
                    if img_arr.shape[2] == 1:
                        img_arr = img_arr.repeat(3, axis=2)
                    Image.fromarray(img_arr).save(os.path.join(temp_gen_fid_dir, f'gen_img_{img_idx}.png'))

                # g. Calculate FID
                print("Calculating FID score...")
                # Note: fid_score handles its own inference context
                current_fid = fid_score.calculate_fid_given_paths(
                    [real_images_for_fid_dir, temp_gen_fid_dir],
                    batch_size=16,
                    device=trainer.accelerator.device,
                    dims=2048
                )
                fid_scores[milestone] = current_fid
                print(f"FID Score at step {milestone}: {current_fid}")

                # h. Cleanup
                shutil.rmtree(temp_gen_fid_dir)

                # FIX 3: Force zero_grad and train mode on main model before resuming
                trainer.opt.zero_grad()
                trainer.model.train()

        trainer.accelerator.wait_for_everyone()

# 8. After the training loop, save the final trained model (EMA model).
final_model_save_path = os.path.join(GD_SAVE_DIR, 'diffusion_model_final.pt')
torch.save(trainer.ema.ema_model.state_dict(), final_model_save_path)
print(f"\nFinal EMA model saved to {final_model_save_path}")

# Print all recorded FID scores
print("\n--- All Recorded FID Scores ---")
for step, fid in fid_scores.items():
    print(f"Step {step}: {fid}")



"""## Generate New MRI Images
Use the trained diffusion model to synthesize a batch of new MRI images.

The `diffusion.sample()` method will be used to generate the images, and the previously defined `show_images` function will be used for visualization.
"""

# Generate images
sampled_images = diffusion.sample(batch_size = 16)

# Display the generated images (requires matplotlib)
import matplotlib.pyplot as plt
import math

def show_images(images, cols=4):
    rows = math.ceil(len(images) / cols)
    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))
    axes = axes.flatten() # Flatten the 2D array of axes for easy iteration

    for i, img in enumerate(images):
        axes[i].imshow(img.permute(1, 2, 0).cpu().numpy())
        axes[i].axis('off')

    # Hide any unused subplots if the number of images is not a perfect multiple of cols
    for j in range(i + 1, len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    plt.show()

show_images(sampled_images, cols=4)

"""The next step is to calculate the FID score between the newly generated images and the original preprocessed MRI images. We did the required setup for FID calculation, including creating temporary directories and resizing real images already.


"""

# calculate FID score. It is great if the score is lower than 20. But we need a long time to train the model
import torch
from pytorch_fid import fid_score
import os
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np

# Assuming 'sampled_images' contains the generated images (PyTorch tensor)
# and the real images are in the folder specified in the trainer

# Convert generated images to the expected format (numpy array, uint8, HxWx3)
# The generated images are likely in the range [-1, 1] and in CxHxW format
generated_images_np = ((sampled_images + 1) * 127.5).clamp(0, 255).permute(0, 2, 3, 1).cpu().numpy().astype('uint8')

# Create temporary directories to save generated and resized real images for FID calculation
temp_gen_dir = './temp_generated_images'
temp_real_dir = './temp_real_images_resized'
os.makedirs(temp_gen_dir, exist_ok=True)
os.makedirs(temp_real_dir, exist_ok=True)


for i, img in enumerate(generated_images_np):
    # Convert grayscale to RGB if needed (FID expects 3 channels)
    if img.shape[2] == 1:
        # Assuming grayscale images are provided as HxWx1 or HxW
        # If HxW, add a channel dimension
        if img.ndim == 2:
            img = img[:, :, np.newaxis]
        img = img.repeat(3, axis=2) # Repeat the single channel 3 times
    plt.imsave(os.path.join(temp_gen_dir, f'gen_img_{i}.png'), img)

# Resize real images to a consistent size and save them
real_image_folder = './converted_lgg_mri_images_png'
image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']

real_image_files = [os.path.join(real_image_folder, f) for f in os.listdir(real_image_folder) if os.path.isfile(os.path.join(real_image_folder, f)) and os.path.splitext(f)[1].lower() in image_extensions]

# Determine a target size (e.g., the size of generated images)
target_size = (generated_images_np.shape[2], generated_images_np.shape[1]) # (width, height)


for i, img_path in enumerate(real_image_files):
    try:
        img = Image.open(img_path)
        # Ensure image has 3 channels
        if img.mode != 'RGB':
            img = img.convert('RGB')
        img_resized = img.resize(target_size)
        img_resized.save(os.path.join(temp_real_dir, f'real_img_{i}.png'))
    except Exception as e:
        print(f"Could not process image {img_path}: {e}")
        continue


# Calculate FID score
# Provide the path to the resized real images folder and the temporary generated images folder
fid_value = fid_score.calculate_fid_given_paths([temp_real_dir, temp_gen_dir],
                                                batch_size=generated_images_np.shape[0],
                                                device='cuda' if torch.cuda.is_available() else 'cpu',
                                                dims=2048)

print(f"FID Score: {fid_value}")

# Clean up the temporary directories
import shutil
shutil.rmtree(temp_gen_dir)
shutil.rmtree(temp_real_dir)

"""**Reasoning**:
The next step is to load the trained U-Net model, preprocess the generated images for segmentation, predict masks using the U-Net, and then visualize the generated images alongside their predicted masks.


"""

import tensorflow as tf
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

# Load the best weights for the UNet model
# Assuming `model` from HTHoUJGcpmHD is the UNet model definition
# and `CHECKPOINT_PATH` is defined as '/kaggle/working/unet_lgg_checkpoint.h5'

# Re-build the UNet model to load weights
def conv_block(x, filters, kernel_size=3, padding="same", activation="relu"):
    x = tf.keras.layers.Conv2D(filters, kernel_size, padding=padding)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation(activation)(x)
    x = tf.keras.layers.Conv2D(filters, kernel_size, padding=padding)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation(activation)(x)
    return x

def encoder_block(x, filters):
    c = conv_block(x, filters)
    p = tf.keras.layers.MaxPooling2D((2,2))(c)
    return c, p

def decoder_block(x, skip, filters):
    us = tf.keras.layers.Conv2DTranspose(filters, (2,2), strides=(2,2), padding="same")(x)
    concat = tf.keras.layers.concatenate([us, skip])
    c = conv_block(concat, filters)
    return c

def build_unet(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 1)):
    inputs = tf.keras.layers.Input(shape=input_shape)
    c1, p1 = encoder_block(inputs, 32)
    c2, p2 = encoder_block(p1, 64)
    c3, p3 = encoder_block(p2, 128)
    c4, p4 = encoder_block(p3, 256)
    bn = conv_block(p4, 512)
    d1 = decoder_block(bn, c4, 256)
    d2 = decoder_block(d1, c3, 128)
    d3 = decoder_block(d2, c2, 64)
    d4 = decoder_block(d3, c1, 32)
    outputs = tf.keras.layers.Conv2D(1, (1,1), activation="sigmoid")(d4)
    model = tf.keras.Model(inputs, outputs, name="UNet")
    return model

# Define IMG_SIZE, if not already defined
# IMG_SIZE = (256, 256) # This should be consistent with the U-Net training

unet_model = build_unet(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 1))

CHECKPOINT_PATH = "/kaggle/working/unet_lgg_checkpoint.h5"
if os.path.exists(CHECKPOINT_PATH):
    print(f"Loading U-Net weights from {CHECKPOINT_PATH}")
    unet_model.load_weights(CHECKPOINT_PATH)
else:
    print(f"U-Net checkpoint not found at {CHECKPOINT_PATH}. Please ensure the U-Net training was successful.")

# Prepare generated images for U-Net segmentation
# sampled_images are PyTorch tensors in range [-1, 1] and CxHxW format (e.g., 16x3x128x128)
# U-Net expects NumPy arrays in range [0, 1] and HxWx1 format (e.g., 256x256x1)

# Resize generated images to U-Net input size (256x256)
# Assuming the generated images are 128x128 (image_size = 128 in diffusion model)
processed_generated_images = []
target_unet_size = IMG_SIZE # (256, 256)

for img_tensor in sampled_images:
    # Convert PyTorch tensor to NumPy array, permute to HxWxC, and normalize to [0, 1]
    img_np = img_tensor.permute(1, 2, 0).cpu().numpy()
    img_np = (img_np + 1) / 2 # Scale from [-1, 1] to [0, 1]

    # If the generated image is RGB (3 channels), convert to grayscale for U-Net input
    if img_np.shape[2] == 3:
        # Convert RGB to grayscale (simple average or luminance conversion)
        img_np = np.mean(img_np, axis=-1, keepdims=True)
    elif img_np.shape[2] == 1: # Already grayscale, just keepdims
        pass
    else:
        raise ValueError(f"Unexpected number of channels in generated image: {img_np.shape[2]}")

    # Resize image using PIL for higher quality interpolation
    img_pil = Image.fromarray((img_np.squeeze() * 255).astype(np.uint8), mode='L') # 'L' for grayscale
    img_resized_pil = img_pil.resize(target_unet_size, Image.BILINEAR)
    img_resized_np = np.array(img_resized_pil).astype(np.float32) / 255.0
    img_resized_np = np.expand_dims(img_resized_np, axis=-1) # Add channel dimension back

    processed_generated_images.append(img_resized_np)

processed_generated_images = np.array(processed_generated_images)

print(f"Shape of processed generated images for U-Net: {processed_generated_images.shape}")

# Predict segmentation masks using the loaded U-Net model
predicted_masks = unet_model.predict(processed_generated_images)
predicted_masks_binary = (predicted_masks >= 0.5).astype(np.float32)

print(f"Shape of predicted masks: {predicted_masks_binary.shape}")

# Visualize a selection of generated images and their predicted masks
def show_generated_and_segmented(generated_imgs, predicted_msks, num_to_show=5):
    plt.figure(figsize=(num_to_show * 4, 8))
    for i in range(num_to_show):
        if i >= len(generated_imgs):
            break

        # Original generated image (before U-Net preprocessing, for display)
        # Scale back to display range if needed (sampled_images was [-1,1])
        display_img_tensor = sampled_images[i]
        display_img_np = (display_img_tensor.permute(1, 2, 0).cpu().numpy() + 1) / 2
        if display_img_np.shape[2] == 3: # if RGB, convert to grayscale for consistent display
             display_img_np = np.mean(display_img_np, axis=-1, keepdims=True)

        # Resizing for display purposes only, to match U-Net input size
        display_img_pil = Image.fromarray((display_img_np.squeeze() * 255).astype(np.uint8), mode='L')
        display_img_resized = np.array(display_img_pil.resize(target_unet_size, Image.BILINEAR)) / 255.0


        plt.subplot(2, num_to_show, i + 1)
        plt.imshow(display_img_resized.squeeze(), cmap='gray')
        plt.title(f"Generated Image {i+1}")
        plt.axis('off')

        plt.subplot(2, num_to_show, num_to_show + i + 1)
        plt.imshow(predicted_msks[i].squeeze(), cmap='gray')
        plt.title(f"Predicted Mask {i+1}")
        plt.axis('off')
    plt.tight_layout()
    plt.show()

show_generated_and_segmented(processed_generated_images, predicted_masks_binary, num_to_show=8)



"""### FID Score Progression
Let's visualize how the FID score improved over the training steps for the diffusion model.
"""

import matplotlib.pyplot as plt

# results from previous run above. Feel free to remove out if you re-run the whole thing.
# fid_scores = {
# 1000: 403.7773854513947,
# 10000: 159.4962088923301,
# 20000: 178.74492584137914,
# 30000: 163.8168105666117,
# 40000: 163.9009578583577,
# 50000: 155.18878896452077
#                }

if fid_scores:
    steps = sorted(fid_scores.keys())
    fids = [fid_scores[step] for step in steps]

    plt.figure(figsize=(10, 6))
    plt.plot(steps, fids, marker='o', linestyle='-', color='blue')
    plt.title('FID Score vs. Diffusion Model Training Steps')
    plt.xlabel('Training Steps')
    plt.ylabel('FID Score')
    plt.grid(True)
    plt.xticks(steps, rotation=45)
    plt.tight_layout()
    plt.show()
else:
    print("No FID scores recorded to plot. Please ensure the diffusion model training with FID calculation was completed.")

"""### Qualitative Assessment of Generated Images and Segmentation

The previous visualizations in the notebook demonstrated the diffusion model's ability to generate new MRI images and the U-Net's performance in segmenting potential tumor regions on these synthetic images.

*   **Diffusion Model Generated Images**: The `show_images` function was used to display samples generated at various checkpoints, allowing for a visual inspection of the image quality as training progressed.
*   **U-Net Segmentation**: The `show_generated_and_segmented` function displayed a selection of the diffusion-generated MRI images alongside their corresponding predicted masks, providing a qualitative assessment of the U-Net's segmentation capabilities on the synthetic data.

### Overall Assessment of Outcomes and Insights

This project successfully demonstrated a pipeline for generating synthetic medical images using a Denoising Diffusion Probabilistic Model (DDPM) and subsequently segmenting features within these generated images using a U-Net architecture.

**Diffusion Model Training & FID:** The training progression of the diffusion model was monitored using FID scores at various checkpoints. As seen in the plot above (if FID scores were successfully recorded and plotted), a decreasing FID score indicates an improvement in the quality and diversity of the generated images, making them more similar to the real dataset. The dynamic progress bar provided real-time feedback, aiding in monitoring the lengthy training process.

**U-Net Segmentation on Synthetic Data:** The U-Net model was capable of predicting masks on the diffusion-generated MRI images. This is a crucial step towards potentially augmenting medical datasets with synthetic, yet realistic, images and then using these images for downstream tasks like training segmentation models. The qualitative visualization of generated images and their predicted masks helps confirm the U-Net's ability to identify relevant structures even in novel, synthetically produced data.

**Insights:** The combination of diffusion models for data generation and U-Net for segmentation presents a powerful approach for medical image analysis, especially in scenarios where real annotated data is scarce. Future work could involve more extensive training of the diffusion model to achieve lower FID scores, further refining the synthetic image quality, and quantitatively evaluating the U-Net's segmentation performance on these synthetic images against actual ground truth masks (if available for synthetic data or by comparing models trained on real vs. synthetic data).


"""

import random
import os
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import math
import torch

# !pip install denoising_diffusion_pytorch
from denoising_diffusion_pytorch import Unet, GaussianDiffusion

def show_images_comparison(real_images, generated_images, num_to_show=4, display_mode='color'):
    rows = num_to_show
    cols = 2 # Always 2 columns: Real and Generated
    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))

    # Ensure axes is always a 2D array for consistent indexing
    if num_to_show == 1: # If only 1 row, subplots returns a 1D array of 2 axes
        axes = np.expand_dims(axes, axis=0) # Make it (1, 2)

    for i in range(num_to_show):
        if i < len(real_images):
            # Process real image for display
            real_img_np = real_images[i] # HxWx1 numpy array [0,1]
            if display_mode == 'color':
                # Convert HxWx1 to HxWx3 by repeating channel for 'color' look
                display_real_img = np.repeat(real_img_np, 3, axis=-1)
                axes[i, 0].imshow(display_real_img)
            else: # grayscale
                axes[i, 0].imshow(real_img_np.squeeze(), cmap='gray')
            axes[i, 0].set_title(f'Real MRI {i+1}')
            axes[i, 0].axis('off')

        if i < len(generated_images):
            # Process generated image for display
            gen_img_tensor = generated_images[i] # CxHxW torch tensor [-1,1]
            gen_img_np = gen_img_tensor.permute(1, 2, 0).cpu().numpy() # HxWxC
            gen_img_np = (gen_img_np + 1) / 2 # Scale to [0,1]

            if display_mode == 'color':
                # Assuming generated are RGB (C=3), display directly
                axes[i, 1].imshow(gen_img_np)
            else: # grayscale
                # Convert RGB generated to grayscale if needed
                if gen_img_np.shape[-1] == 3:
                    gen_img_np = np.mean(gen_img_np, axis=-1, keepdims=True)
                axes[i, 1].imshow(gen_img_np.squeeze(), cmap='gray')
            axes[i, 1].set_title(f'Generated MRI {i+1}')
            axes[i, 1].axis('off')

    plt.suptitle(f'MRI Image Comparison ({display_mode.capitalize()})', fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make space for suptitle
    plt.show()

# --- Load and preprocess real images for comparison ---
# trainer_data_folder should point to './converted_lgg_mri_images_png'
# IMG_SIZE_DIFFUSION should be (128, 128)

# Redefine trainer_data_folder and IMG_SIZE_DIFFUSION for this cell
trainer_data_folder = '/content/drive/MyDrive/ColabNotebooks/EnhanceMRIdata/real_images_for_fid'
IMG_SIZE_DIFFUSION = (128, 128)

# Get a list of all real image paths
real_image_files = [os.path.join(trainer_data_folder, f)
                    for f in os.listdir(trainer_data_folder)
                    if os.path.isfile(os.path.join(trainer_data_folder, f))]

# Randomly select a subset of real images
num_real_to_sample = 4 # Match with num_to_show for generated images
random_real_paths = random.sample(real_image_files, min(num_real_to_sample, len(real_image_files)))

selected_real_images = []
for img_path in random_real_paths:
    try:
        img = Image.open(img_path)
        # Convert to grayscale if not already, and resize to diffusion model's input size
        img = img.convert('L') # Convert to grayscale
        img = img.resize(IMG_SIZE_DIFFUSION, Image.BILINEAR)
        img_np = np.array(img).astype(np.float32) / 255.0
        img_np = np.expand_dims(img_np, axis=-1) # Add channel dimension
        selected_real_images.append(img_np)
    except Exception as e:
        print(f"Error loading real image {img_path}: {e}")
        continue

# Convert list of numpy arrays to a batch for consistent display
selected_real_images_batch = np.stack(selected_real_images, axis=0)

# --- Load the saved diffusion model and generate new images ---
# Re-instantiate the Unet model (make sure it's the same as the one you trained)
model = Unet(
    dim = 64,
    dim_mults = (1, 2, 4, 8),
    flash_attn = False,
    channels = 3 # Explicitly set channels to 3 for RGB images
)

diffusion = GaussianDiffusion(
    model,
    image_size = 128,
    timesteps = 1000,
    sampling_timesteps = 100
)

# Define the path to your final saved model checkpoint
final_model_save_path = '/content/drive/MyDrive/ColabNotebooks/EnhanceMRIdata/diffusion_model_final.pt'

# Load the saved state dictionary into the model
device = 'cuda' if torch.cuda.is_available() else 'cpu'
if os.path.exists(final_model_save_path):
    print(f"Loading final diffusion model from {final_model_save_path}")
    checkpoint = torch.load(final_model_save_path, map_location=device)
    diffusion.load_state_dict(checkpoint) # Load into diffusion object
    diffusion.to(device)
    diffusion.eval() # Set diffusion model to evaluation mode
else:
    raise FileNotFoundError(f"Final diffusion model not found at {final_model_save_path}. Please ensure training and saving were successful.")

# Generate new images using the loaded model
print("Generating sampled images from the loaded diffusion model...")
with torch.no_grad(): # Disable gradient calculations for inference
    sampled_images = diffusion.sample(batch_size = 16) # Generate 16 images for comparison

# Select the first 'num_real_to_sample' generated images for comparison
selected_generated_images = sampled_images[:num_real_to_sample]

# Display the comparison in color mode
print("Displaying a comparison of real vs. generated MRI images (Color):")
show_images_comparison(selected_real_images_batch, selected_generated_images, num_to_show=num_real_to_sample, display_mode='color')

# Display the comparison in grayscale mode
print("\nDisplaying a comparison of real vs. generated MRI images (Grayscale):")
show_images_comparison(selected_real_images_batch, selected_generated_images, num_to_show=num_real_to_sample, display_mode='grayscale')